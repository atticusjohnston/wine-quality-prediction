{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.905100Z",
     "start_time": "2024-04-29T15:42:34.159602Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.907999Z",
     "start_time": "2024-04-29T15:42:35.905981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def notify(title, text, model):\n",
    "    os.system(\"\"\"\n",
    "              osascript -e 'display notification \"{}\" with title \"{}\"'\n",
    "              \"\"\".format(text, title))\n",
    "    # os.system(f'say \"{model.__class__.__name__} training complete!\"')"
   ],
   "id": "99ba406598c75c72",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.914856Z",
     "start_time": "2024-04-29T15:42:35.908471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rawWhitesDF = pd.read_csv(\"raw-wine-data/winequality-white.csv\", delimiter=\";\")\n",
    "rawRedsDF = pd.read_csv(\"raw-wine-data/winequality-red.csv\", delimiter=\";\")"
   ],
   "id": "d7bdcadd3cb71efb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.923829Z",
     "start_time": "2024-04-29T15:42:35.915391Z"
    }
   },
   "cell_type": "code",
   "source": "rawWhitesDF.head()",
   "id": "de6c52d8b07f2a33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.935373Z",
     "start_time": "2024-04-29T15:42:35.925217Z"
    }
   },
   "cell_type": "code",
   "source": "print(rawWhitesDF.describe())",
   "id": "ef0384020d76e6bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
      "mean        6.854788          0.278241     0.334192        6.391415   \n",
      "std         0.843868          0.100795     0.121020        5.072058   \n",
      "min         3.800000          0.080000     0.000000        0.600000   \n",
      "25%         6.300000          0.210000     0.270000        1.700000   \n",
      "50%         6.800000          0.260000     0.320000        5.200000   \n",
      "75%         7.300000          0.320000     0.390000        9.900000   \n",
      "max        14.200000          1.100000     1.660000       65.800000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
      "mean      0.045772            35.308085            138.360657     0.994027   \n",
      "std       0.021848            17.007137             42.498065     0.002991   \n",
      "min       0.009000             2.000000              9.000000     0.987110   \n",
      "25%       0.036000            23.000000            108.000000     0.991723   \n",
      "50%       0.043000            34.000000            134.000000     0.993740   \n",
      "75%       0.050000            46.000000            167.000000     0.996100   \n",
      "max       0.346000           289.000000            440.000000     1.038980   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
      "mean      3.188267     0.489847    10.514267     5.877909  \n",
      "std       0.151001     0.114126     1.230621     0.885639  \n",
      "min       2.720000     0.220000     8.000000     3.000000  \n",
      "25%       3.090000     0.410000     9.500000     5.000000  \n",
      "50%       3.180000     0.470000    10.400000     6.000000  \n",
      "75%       3.280000     0.550000    11.400000     6.000000  \n",
      "max       3.820000     1.080000    14.200000     9.000000  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c247a880445e803"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.944450Z",
     "start_time": "2024-04-29T15:42:35.935980Z"
    }
   },
   "cell_type": "code",
   "source": "print(rawRedsDF.describe())",
   "id": "e6e3e785535bca7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.950627Z",
     "start_time": "2024-04-29T15:42:35.945195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "featureLabels = list(rawWhitesDF.columns.values)\n",
    "classes = [c for c in range(1, 11)]\n",
    "\n",
    "# Normalising the continuous data with MinMax scaling\n",
    "scaledWhitesDF = rawWhitesDF.copy()\n",
    "scaledRedsDF = rawRedsDF.copy()\n",
    "\n",
    "for column in featureLabels[:-1]:\n",
    "    scaledWhitesDF[column] = (scaledWhitesDF[column] - scaledWhitesDF[column].min()) / (scaledWhitesDF[column].max() - scaledWhitesDF[column].min())\n",
    "    scaledRedsDF[column] = (scaledRedsDF[column] - scaledRedsDF[column].min()) / (scaledRedsDF[column].max() - scaledRedsDF[column].min())"
   ],
   "id": "e8d4dfecea288533",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.959692Z",
     "start_time": "2024-04-29T15:42:35.951168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whiteTargetsInt = torch.tensor(scaledWhitesDF['quality'].to_numpy())\n",
    "whiteFeatures = torch.tensor(scaledWhitesDF[featureLabels[0:-1]].to_numpy(), requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "redTargetsInt = torch.tensor(scaledRedsDF['quality'].to_numpy())\n",
    "redFeatures = torch.tensor(scaledRedsDF[featureLabels[0:-1]].to_numpy(), requires_grad=True, dtype=torch.float32)"
   ],
   "id": "6653419db253c007",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.993254Z",
     "start_time": "2024-04-29T15:42:35.960568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whiteTargets =  []\n",
    "\n",
    "for i, target in enumerate(whiteTargetsInt):\n",
    "    embed = torch.zeros(len(classes), dtype=torch.float32)\n",
    "    embed[target.item()-1] = 1\n",
    "    whiteTargets.append(embed)\n",
    "whiteTargets = torch.stack(whiteTargets).to(torch.float32)\n",
    "\n",
    "redTargets = []\n",
    "\n",
    "for i, target in enumerate(redTargetsInt):\n",
    "    embed = torch.zeros(len(classes), dtype=torch.float32)\n",
    "    embed[target.item()-1] = 1\n",
    "    redTargets.append(embed)\n",
    "redTargets = torch.stack(redTargets).to(torch.float32)"
   ],
   "id": "40f6e671c072ec42",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:35.995876Z",
     "start_time": "2024-04-29T15:42:35.994003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'features': self.features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "        return sample"
   ],
   "id": "932790ca7a7c8456",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:36.000159Z",
     "start_time": "2024-04-29T15:42:35.996507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_indices, test_indices = train_test_split(range(len(whiteFeatures)), test_size=0.2, random_state=22)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.1, random_state=22)"
   ],
   "id": "15c63b5b1388833c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:36.006091Z",
     "start_time": "2024-04-29T15:42:36.000789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = WineDataset(whiteFeatures[train_indices], whiteTargets[train_indices])\n",
    "val_dataset = WineDataset(whiteFeatures[val_indices], whiteTargets[val_indices])\n",
    "test_dataset = WineDataset(whiteFeatures[test_indices], whiteTargets[test_indices])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "red_test_dataset = WineDataset(redFeatures, redTargets)\n",
    "red_test_loader = DataLoader(dataset=red_test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "474b067400571750",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:36.011040Z",
     "start_time": "2024-04-29T15:42:36.006755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_channels, output_size, hidden_size):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64, hidden_size)  \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        # Convolutional layers\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        # Flatten\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, num_heads, hidden_size, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = x.unsqueeze(1) # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.squeeze(1) # (batch_size, hidden_size)\n",
    "        \n",
    "        # Apply output layer\n",
    "        x = self.relu(self.output_layer(x))\n",
    "\n",
    "        return x"
   ],
   "id": "9bef2aef72b9e27",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:36.018715Z",
     "start_time": "2024-04-29T15:42:36.014358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, device, train_loader, optimiser, nepoch, scheduler_step_size, scheduler_gamma):\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # for epoch in range(nepoch):\n",
    "    for epoch in tqdm(range(nepoch), leave=False, unit='epoch', desc= \"Epochs\"):\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        train_count = 0\n",
    "        total_val_loss = 0\n",
    "        val_count = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch['features'], batch['target']\n",
    "            inputs, targets = inputs.to(device).detach(), targets.to(device).detach()\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            train_loss = criterion(outputs, targets)\n",
    "            train_loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            total_train_loss += train_loss.item()\n",
    "            train_count += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch['features'], batch['target']\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                val_loss = criterion(outputs, targets)\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "                val_count += 1\n",
    "        \n",
    "        train_losses.append(total_train_loss/train_count)\n",
    "        val_losses.append(total_val_loss/val_count)\n",
    "        scheduler.step()\n",
    "    \n",
    "    plt.semilogy(train_losses)\n",
    "    plt.semilogy(val_losses)\n",
    "    plt.legend([\"Train loss\", \"Val. loss\"])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    notify(\"Training Finished\", \"\", model)\n",
    "    # print(\"Final training loss:  \", round(train_losses[-1], 4))\n",
    "    # print(\"Final validation loss:\", round(val_losses[-1], 4))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.to(device)\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch['features'], batch['target']\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # predicted = torch.argmax(outputs, dim=1)\n",
    "            predicted = torch.topk(outputs, 2, dim=1).indices\n",
    "            actual = torch.argmax(targets, dim=1)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += torch.sum(torch.any(predicted.eq(actual.unsqueeze(1)), dim=1)).item()\n",
    "            \n",
    "    acc = round(100 * correct / total, 4)\n",
    "    return acc"
   ],
   "id": "2d1b677378fc6712",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T15:42:38.324081Z",
     "start_time": "2024-04-29T15:42:36.019549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseModel = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=6, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "baseOptimiser = torch.optim.Adam(baseModel.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    "scheduler_step_size = 100\n",
    "scheduler_gamma = 0.1\n",
    "nepoch = 300\n",
    "\n",
    "train(baseModel, device, train_loader, baseOptimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "9ffa7a01f42acd3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epochs:   0%|          | 0/300 [00:00<?, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87db5e7c791540e39451c4f49bb1d74d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m scheduler_gamma \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m\n\u001B[1;32m     10\u001B[0m nepoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m\n\u001B[0;32m---> 12\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbaseModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbaseOptimiser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler_step_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler_gamma\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 26\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, device, train_loader, optimiser, nepoch, scheduler_step_size, scheduler_gamma)\u001B[0m\n\u001B[1;32m     24\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[1;32m     25\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 26\u001B[0m \u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m total_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m train_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     29\u001B[0m train_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Documents/wine-quality-prediction/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:75\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m instance\u001B[38;5;241m.\u001B[39m_step_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     74\u001B[0m wrapped \u001B[38;5;241m=\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(instance, \u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/wine-quality-prediction/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/wine-quality-prediction/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/Documents/wine-quality-prediction/.venv/lib/python3.12/site-packages/torch/optim/adam.py:166\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    155\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    157\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    158\u001B[0m         group,\n\u001B[1;32m    159\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    163\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    164\u001B[0m         state_steps)\n\u001B[0;32m--> 166\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/Documents/wine-quality-prediction/.venv/lib/python3.12/site-packages/torch/optim/adam.py:316\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 316\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/wine-quality-prediction/.venv/lib/python3.12/site-packages/torch/optim/adam.py:441\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    439\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m--> 441\u001B[0m     \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_complex(params[i]):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the base model on withheld white wine data.\n",
    "white_base_acc = test(baseModel, device, test_loader)\n",
    "print(\"Base model top-2 accuracy on white wine dataset:\", white_base_acc)"
   ],
   "id": "7fa2d2ed08a9d9de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Printing model summary\n",
    "import torchinfo\n",
    "print(torchinfo.summary(baseModel, (64, 11), col_names = (\"input_size\", \"output_size\", \"num_params\")))"
   ],
   "id": "b492c071ad6a412f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Computational Experiment 1: Testing base model generalisability on red wine dataset.",
   "id": "69e193f2e30013c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the model on red wine data.\n",
    "red_base_acc = test(baseModel, device, red_test_loader)\n",
    "print(\"Base model top-2 accuracy on red wine dataset:\", red_base_acc)"
   ],
   "id": "d491cfe08e04fd13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Computational Experiment 2: Comparing performance of transformer, feed-forward and CNN.",
   "id": "ce3178a3b98e689c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "FFModel = FeedForwardModel(input_size=whiteFeatures.shape[1], output_size=len(classes),hidden_size=128)\n",
    "FFOptimiser = torch.optim.Adam(FFModel.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "train(FFModel, device, train_loader, FFOptimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "a1ebc58fe6bc58a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the feed-forward model on withheld white wine data.\n",
    "white_FF_acc = test(FFModel, device, test_loader)\n",
    "print(\"Feed-forward model top-2 accuracy on white wine dataset:\", white_FF_acc) # "
   ],
   "id": "6fb6a62f9c42d3b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pretty similar. Makes sense since Transformers are best at sequential because of the multi-head attention. Might not even be worth using transformers because of the much longer training time.  \\\n",
    "    - ~6 mins to train transformer \\\n",
    "    - "
   ],
   "id": "e7c5f51cfec8fc2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CNNModel = ConvModel(input_channels=1, output_size=len(classes), hidden_size=128)\n",
    "CNNOptimiser = torch.optim.Adam(CNNModel.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "train(CNNModel, device, train_loader, CNNOptimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "5e5cbd6b70f24a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the CNN model on withheld white wine data.\n",
    "white_CNN_acc = test(CNNModel, device, test_loader)\n",
    "print(\"CNN model top-2 accuracy on white wine dataset:\", white_CNN_acc) # "
   ],
   "id": "25acda5ed03c3420",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You could call the three all even since I spent the most time toying with the transformer model hyperparameters trying to get it optimal. Not how well each of them works, its how well they work in unison.",
   "id": "3ab0e9f5d4229015"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Computational Experiment 3: ",
   "id": "738199fae4ee79df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dropout at 35%",
   "id": "5b62e786cdbc343c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "drop35Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=6, num_heads=16, hidden_size=128, dropout=0.35)\n",
    "drop35Optimiser = torch.optim.Adam(drop35Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(drop35Model, device, train_loader, drop35Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "f70dffdb4076d502",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the dropout-20% model on withheld white wine data.\n",
    "white_drop35_acc = test(drop35Model, device, test_loader)\n",
    "print(\"Dropout-35% model top-2 accuracy on white wine dataset:\", white_drop35_acc)"
   ],
   "id": "88230570ef137ede",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing dropout at 20%.",
   "id": "1c9d04c3e2979d92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "drop20Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=6, num_heads=16, hidden_size=128, dropout=0.2)\n",
    "drop20Optimiser = torch.optim.Adam(drop20Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(drop20Model, device, train_loader, drop20Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "1ebb754b8637369d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the dropout-20% model on withheld white wine data.\n",
    "white_drop20_acc = test(drop20Model, device, test_loader)\n",
    "print(\"Dropout-20% model top-2 accuracy on white wine dataset:\", white_drop20_acc)"
   ],
   "id": "4499e75fcf147c58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Variations in number of layers",
   "id": "f14975c08b1576b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden3Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=3, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "hidden3Optimiser = torch.optim.Adam(hidden3Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(hidden3Model, device, train_loader, hidden3Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "ab921823723ef468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the hidden-4 model on withheld white wine data.\n",
    "white_hidden3_acc = test(hidden3Model, device, test_loader)\n",
    "print(\"Hidden-3 model top-2 accuracy on white wine dataset:\", white_hidden3_acc)"
   ],
   "id": "d8f9cc93e0543d93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4 hidden layers",
   "id": "b68b31fdf57aecdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden4Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=4, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "hidden4Optimiser = torch.optim.Adam(hidden4Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(hidden4Model, device, train_loader, hidden4Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "48225677a712f619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the hidden-4 model on withheld white wine data.\n",
    "white_hidden4_acc = test(hidden4Model, device, test_loader)\n",
    "print(\"Hidden-4 model top-2 accuracy on white wine dataset:\", white_hidden4_acc)"
   ],
   "id": "fb9bc161ecfdb20a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5 hidden layers",
   "id": "7117c58430927fa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden5Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=5, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "hidden5Optimiser = torch.optim.Adam(hidden5Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(hidden5Model, device, train_loader, hidden5Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "50923576ef9f5cdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the hidden-5 model on withheld white wine data.\n",
    "white_hidden5_acc = test(hidden5Model, device, test_loader)\n",
    "print(\"Hidden-5 model top-2 accuracy on white wine dataset:\", white_hidden5_acc)"
   ],
   "id": "1606cd4b36ad7283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6 hidden layers",
   "id": "1123b32c64c30b80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden6Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=6, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "hidden6Optimiser = torch.optim.Adam(hidden6Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(hidden6Model, device, train_loader, hidden6Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "7903666429933ac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the hidden-5 model on withheld white wine data.\n",
    "white_hidden6_acc = test(hidden6Model, device, test_loader)\n",
    "print(\"Hidden-6 model top-2 accuracy on white wine dataset:\", white_hidden6_acc)"
   ],
   "id": "606dbe1fb33e6eef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7 hidden layers",
   "id": "66934466a7e36e1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden7Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=7, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "hidden7Optimiser = torch.optim.Adam(hidden7Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(hidden7Model, device, train_loader, hidden7Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "7fbd9d54fc1109f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the hidden-5 model on withheld white wine data.\n",
    "white_hidden7_acc = test(hidden7Model, device, test_loader)\n",
    "print(\"Hidden-7 model top-2 accuracy on white wine dataset:\", white_hidden7_acc)"
   ],
   "id": "e193b6d7cdee3e8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8 hidden layers",
   "id": "bca95a931117210a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden8Model = TransformerModel(input_size=whiteFeatures.shape[1], output_size=len(classes), num_layers=8, num_heads=16, hidden_size=128, dropout=0.5)\n",
    "hidden8Optimiser = torch.optim.Adam(hidden8Model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train(hidden8Model, device, train_loader, hidden8Optimiser, nepoch, scheduler_step_size, scheduler_gamma)"
   ],
   "id": "c6dd22576e61a784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing the hidden-5 model on withheld white wine data.\n",
    "white_hidden8_acc = test(hidden8Model, device, test_loader)\n",
    "print(\"Hidden-8 model top-2 accuracy on white wine dataset:\", white_hidden8_acc)"
   ],
   "id": "e72afc92c5f315d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
